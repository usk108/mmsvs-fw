# マルチモーダル発話可視化アプリケーション

以下の論文において提案しているマルチモーダル発話可視化アプリケーションのプロトタイプです。
- [聴覚障害者支援のためのマルチモーダル発話可視化に関する研究](http://www27.cs.kobe-u.ac.jp/achieve/data/pdf/1207.pdf)
- [聴覚障害者支援のための発話可視化モーダルの開発支援](http://www27.cs.kobe-u.ac.jp/achieve/data/pdf/1223.pdf)

現在も開発中のものになります．

[デモ映像](https://youtu.be/Ruq-6e94fko)

![全体図](https://github.com/usk108/mmsvs-fw/wiki/images/overview.jpg)

## 研究の概要
詳細は論文で説明しているので概要のみ説明します。

### 実現したい世界
「聴覚に障害があっても複数の健聴者との会話に参加できるようにしたい」

### 背景
健聴者が多数である企業で働く聴覚障害者が増える一方で，
複数の健聴者との会話が難しいということが課題になっています．

### キーアイデア
発話者の音声や口型，その他発話理解に役立つ様々な情報を可視化したもの（可視化モード）を
組み合わせて発話を多面的に可視化し，発話理解を支援することをマルチモーダル発話可視化と呼びます．
たとえば音声認識を使った発話内容のテキスト化モードや、専門用語の説明モードなどを利用者自身が選択することで、様々な聴覚障害のレベルやニーズに合わせた情報保障を実現できると考えます。

![マルチモーダル発話可視化](https://github.com/usk108/mmsvs-fw/wiki/images/mmsv.jpg)

このキーアイデアに加え，聴覚特別支援学校の教員らから得た知見を機能として盛り込み，
開発しているのがマルチモーダル発話可視化アプリケーションです．

### 開発の方針
本アプリケーションは提案手法であるマルチモーダル発話可視化アプリケーションの実用化のために，
アイデアの有用性を検証することを目的として開発されています．
したがって以下の要素がこのプロトタイプで実現されるべき要素となります．
- 可視化モードを使用，及び自由な追加が可能なフレームワークの開発
- 検証が可能となる程度の種類の可視化モードの開発
- 継続的な支援のための可視化モードの開発支援策を盛り込んだ設計

一方で，以下の事項は開発の範疇ではありません．
- 必要とされる全ての可視化モードの開発
- 音声認識精度向上のための研究

## 実装状況
### 可視化モード
#### 顔表示モード
口話法の補助として，端末のカメラから取得した発話者の口元の映像を表示します．

このモードは話者用バージョンと聞き手バージョンが存在します．
1. 話者バージョンではrunボタンを押すことで端末のカメラから話者の口元を取得し，
その映像を自身のアプリ上で確認することができます．
2. 1と同時にその映像をWebSocketサーバを通して聞き手バージョンに送信する状態になります．
3. 聞き手バージョンでrunボタンを押すことで受け取った映像をアプリ上に表示する状態にします．

![顔表示モード](https://github.com/usk108/mmsvs-fw/wiki/images/face_mode.jpg)

#### 音声テキスト化モード
健聴者の発話内容を音声認識処理を通すことで文字情報として可視化します．

1. runボタンで音声認識待機状態となります．
2. １つの音声認識が終了した段階で全クライアントに認識結果のテキストを送信します．
このときテキストはチャット形式で表示されます．
3. stopで音声認識待機状態を解除します．

無償で利用可能な音声認識エンジンとして， Google の Web Speech API を使用しています．

![音声テキスト化モード](https://github.com/usk108/mmsvs-fw/wiki/images/face_mode.jpg)

#### 用語解説モード
専門用語・固有名詞など口話法での読み取りが困難な単語を指定することで，その意味情報を表示できます．

1. 音声テキストモードなどによって画面上に表示された文字列の中から調べたい用語上でドラッグして選択状態にします
2. runボタンを押します
3. 単語の意味情報が表示されます

無償で利用可能な用語検索ツールとして，フリー百科事典である Wikipedia からの用語検索が可能な MediaWikiAPIを使用している．


![用語解説モード](https://github.com/usk108/mmsvs-fw/wiki/images/face_mode.jpg)

## 工夫した点
### 可視化モードがプラグインのように拡張可能な設計
複数の情報提示・可視化機能を聴覚障害者自身がその必要性に応じて選択するために、多様な可視化モードの実現が必要です。
そのために制御の反転をとりいれたアーキテクチャを採用することにより開発の容易化をしたり、
Pub/Subメッセージングモデルを使った可視化モードの組み合わせによるより高機能な機能の開発を可能にしています。

### クライアント間通信をWebSocketに統一
顔映像をクライアント間でやりとりする際，当初はWebRTCを用いていました．
一方でテキストデータなどその他の情報はWebSocket通信で行われていました．
しかしWebRTCでは実装が複雑化してしまう複数での通信が想定されることや，
今後の開発の容易化を考えて設計をシンプルにしたいということを考慮し，
WebSocket通信に一本化しました．
画像1つ1つをバイナリーに変換し送信するという方法で実現しました．


## 使用技術
### 言語
- Javascript
    - nodejs
- html, css
- Java

### API
- Web Speech API
    - Googleが提供している無償の音声認識エンジン
- MediaWikiAPI
    - フリー百科事典である Wikipedia からの用語情報の取得が可能

### その他
- WebSocket
- [BinaryJS](http://binaryjs.com/)
